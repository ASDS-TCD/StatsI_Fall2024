308+1+20+
43+4+0 +
175+2+84 +
341+3+20 +
651+11+15+
296+8+46 +
613+5+160+
757+5+140+280+1+0
set . seed (123)
data <− data.frame(x = runif(200, 1, 10)) data$y <− 0 + 2.75∗data$x + rnorm(200, 0, 1.5)
set.seed(123)
data <-data.frame(x = runif(200, 1, 100))
data$y <- 0 + 2.75*data$x +rnorm(200, 0, 1.5)
# write the function of the OLS regression with θ=(β, σ)
linear.lik <- function(theta, y, x){
n <- nrow(x)
k <- ncol(x)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y-x%*%beta
logl <- -0.5*n*log(2*pi)-0.5*n*log(sigma2)-((t(e)%*%e)/(2*sigma2))
return(-logl)
}
linear.MLE <- optim(fn=linear.lik, par = c(1,1,1), hessian = TRUE, y = data$y, x = cbind(1, data$x), method = "BFGS")
linear.MLE$par
?data("infants")
df_s2 <- data.frame(out)
> View(df_s2)
> plot(df_s1)
> out = tab_Q_summary %>%
+   arrange(outcome) %>%
+   mutate(Correct = recode(corr_w1, `-1`="All", `0`="Incorrect", `1`="Correct", `3`="zDinD"),
+          column = paste(outcome, Correct, sep = "_")
+   ) %>%
+   select(-(statistic:corr_w1), -term, -Correct) %>%
+   gather(variable, value, estimate, std.error) %>%
+   mutate(
+     value = format_num(value, 3),
+     value = ifelse(variable == "std.error", paste0("(", value, ")"), value),
+     Congenial = ifelse(is.na(Congenial), "All", as.character(Congenial))
+   ) %>%
+   spread(column, value) %>%
+   select(-variable) %>%
+   select(Category, Congenial, corr_w1_All, cert_w1_Correct, pInitial_cross_Correct, zDiff_Correct, cert_w1_Incorrect, pInitial_cross_Incorrect, zDiff_Incorrect, zDiD_zDinD) %>%
+   mutate(Category = as.character(Category))
>
>
> for(i in nrow(out):2){
+   if(out$Category[i]==out$Category[i-1]) out$Category[i] = ""
+ }
>
> for(i in ((1:(nrow(out)/2))*2-1)){
+   if(out$Congenial[i] == "All"){
+     out$Congenial[i] = "All responses"
+     out$Congenial[i+1] = ""
+   }
+   if(out$Congenial[i] == "Congenial"){
+     out$Congenial[i] = "Correct ans."
+     out$Congenial[i+1] = "is congenial"
+   }
+   if(out$Congenial[i] == "Uncongenial"){
+     out$Congenial[i] = "Incorrect ans."
+     out$Congenial[i+1] = "is congenial"
+   }
+   if(out$Category[i] == "Political knowledge"){
+     out$Category[i] = "Political"
+     out$Category[i+1] = "knowledge"
+   }
+   if(out$Category[i] == "Controversies"){
+     out$Category[i] = "Contro-"
+     out$Category[i+1] = "versies"
+   }
+ }
> View(out)
> View(df_s2)
> out = tab_cross_stabil %>%
+   group_by()  %>%
+   filter(!is.na(Congenial),
+          variable == "pInitial_cross") %>%
+   group_by() %>%
+   left_join(tab_cross_stabil_N %>% filter(
+     variable == "pInitial_cross")
+   ) %>%
+   filter(Date == "March-August 2020") %>%
+   mutate(
+     N = ifelse(is.na(N), 0, N)
+   ) %>%
+   mutate(
+     CI   = paste0("(", format_num(conf.low), ", ", format_num(conf.high), ")"),
+     N = as.character(round(N))
+   ) %>%
+   select(Survey, Category, Congenial, Correct, Certainty=cert_bin_w1, Estimate=estimate, SE=std.error, CI, N)  %>%
+   arrange(Survey, Category, Congenial, Correct, Certainty) %>%
+   mutate(Certainty = recode(Certainty, `0.5`="0.5", `0.55`="[0.51,0.59]", `0.65`="[0.6,0.69]", `0.75`="[0.7,0.79]", `0.85`="[0.8,0.89]", `0.95`="[0.9,0.99]", `1`="1"),
+          Survey = gsub("MTurk, ", "", Survey),
+          Category = ifelse(Congenial == "Political knowledge", "Political knowledge", Category),
+          Congenial = recode(Congenial, `Political knowledge`="Not applicable"))
df_s1 <- data.frame(tab_var$cert_bin_w1, tab_var$estimate)
df_s2 <- data.frame(s2$Certainty, s2$Estimate)
df_s2["2", "s2.Certainty"] <- 0.55
df_s2["3", "s2.Certainty"] <- 0.65
df_s2["4", "s2.Certainty"] <- 0.75
df_s2["5", "s2.Certainty"] <- 0.85
df_s2["6", "s2.Certainty"] <- 0.95
df_s2["9", "s2.Certainty"] <- 0.55
df_s2["10", "s2.Certainty"] <- 0.65
df_s2["11", "s2.Certainty"] <- 0.75
df_s2["12", "s2.Certainty"] <- 0.85
df_s2["13", "s2.Certainty"] <- 0.95
lm_st <- lm(df_s2$s2.Certainty ~ df_s2$s2.Estimate)
lm_s1 <- lm(df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate)
abline(lm(df_s2$s2.Certainty ~ df_s2$s2.Estimate))
abline(lm(df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate))
plot(lm(df_s2$s2.Certainty ~ df_s2$s2.Estimate))
plot(lm(df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate))
summary(lm_st)
lm_s1 <- lm(formula = df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate)
lm_s2 <- lm(formula = df_s2$s2.Certainty ~ df_s2$s2.Estimate)
t.test(lm_s1, lm_st, paired = TRUE)
mean(df_s1$tab_var.cert_bin_w1)
mean(df_s2$s2.Certainty)
library(gplots)
plotmeans(tab_var.cert_bin_w1 ~ tab_var.estimate, data = df_s1, frame = FALSE)
plotmeans(s2.Certainty ~ s2.Estimate, data = df_s2, frame= FALSE)
df_s2$s2.Certainty <- as.numeric(df_s2$s2.Certainty)
ggplot(anova(lm_s2, lm_s1))
newvalsforx <- function(x) {
xrng <- seq(min(x), max(x), length.out=100)
function(m) data.frame(x=xrng, y=predict(m, data.frame(x=xrng)))
}
ggplot(d, aes("Expected", "Certainty")) +
geom_point() +
geom_line(data= lm_st, color="red") +
geom_line(data= lm_s1, color="blue")
ggplot(df_s2, aes(lm_s1, lm_s2))
table(df_s1)
abline(lm(lm_s1))
x1 <- df_s1$tab_var.cert_bin_w1
y1 <- df_s1$tab_var.estimate
plot(x1, y1, main = "Respondent Certitude_Incentivized",
xlab = "Certainty", ylab = "Estimate",
pch = 19, frame = FALSE)
abline(lm(y1 ~ x1, data = df_s1), col = "blue")
x2 <- df_s2$s2.Certainty
y2 <- df_s2$s2.Estimate
plot(x2, y2, main = "Respondent Certitude_2-Wave",
xlab = "Certainty", ylab = "Estimate",
pch = 19, frame = FALSE)
abline(lm(y2 ~ x2, data = df_s2), col = "blue")
x <- rnorm(100, 0, 1)
rm(x)
x <- rnorm(100, 0, 1)
?merge
?mean
x <- rnorm(100, 0, 1)
y <- rnorm(100, 0, 1)
plot(x, y)
77001 23 +
y <- c(rep("small", sampe(1, 10:20)), rep("med", sampe(1, 10:20)), rep("large", sampe(1, 10:20)), rep("jumbo", sampe(1, 10:20)))
y <- c(rep("small", sample(1, 10:20)), rep("med", sample(1, 10:20)), rep("large", sample(1, 10:20)), rep("jumbo", sample(1, 10:20)))
?sample
y <- c(rep("small", sample(10:20, 1)), rep("med", sample(10:20, 1)), rep("large", sample(10:20, 1)), rep("jumbo", sample(10:20, 1)))
y
length(y)
table(y)
?mode
mode(y)
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(y)
mean(y)
median(y)
x=c(rnorm(18, 0, 1), rnorm(18, 0, 1), rnorm(18, 0, 1), rnorm(18, 0, 1))
test_df <- data.frame(y=c(rep("small", 18), rep("med", 13), rep("large", 14), rep("jumbo", 19)),
test_df <- data.frame(y=c(rep("small", 18), rep("med", 13), rep("large", 14), rep("jumbo", 19)),
x=c(rnorm(18, 0, 1), rnorm(18, 0, 1), rnorm(18, 0, 1), rnorm(18, 0, 1)))
test_df <- data.frame(y=c(rep("small", 18), rep("med", 13), rep("large", 14), rep("jumbo", 19)),
x=c(rnorm(18, 0, 1), rnorm(13, 0, 1), rnorm(14, 0, 1), rnorm(19, 0, 1)))
View(test_df)
library(ggplot2)
ggplot(test_df, aes(x = x, fill = y)) +
geom_histogram(binwidth = .5, alpha =.5, position = "identity")
aggregate(. ~ y, test_df, function(x) c(mean = mean(x), sd = sd(x)))
aggregate(. ~ y, test_df, function(x) c(mean = mean(x), median = median(x), sd = sd(x)))
-1.16+(6.21*1)-(3.82∗2.2) -(1.2*1*2.2)
-1.16+(6.21*1)-(3.82*2.2) -(1.2*1*2.2)
1.16+(6.21*1)-(3.82*0.36) −(1.2*1*0.36)
1.16+(6.21*1)-(3.82*0.36)-(1.2*1*0.36)
-1.16+(6.21*1)-(3.82*2.2) -(1.2*1*2.2)
-1.16+(6.21*1)-(3.82*0.36)-(1.2*1*0.36)
estimate <- -4.238
standard_Error <- 1.848
coefficient <- c('education')
# tscores <- (Estimates-0)/(Standard_Errors)
# p_values <- 2*pt(abs(tscores) , 131-3, lower.tail = F)
model_results <- data.frame(coefficient, estimate, standard_Error)
print(model_results)
CI <-
data.frame("low_CI" = (model_results$estimate - abs(qt((1-.95)/2, df=98))*
model_results$standard_Error),
"estimate" = model_results$estimate,
"high_Ci" = (model_results$estimate + abs(qt((1-.95)/2, df=98))*
model_results$standard_Error))
print(CI)
# Y = 42.39 -2(GDP) -1.303(Dem)-4.238(Edu)
# Y = 42.39 -2(GDP)- 4.238(12.05)
- 4.238*(12.05) #-51.0679
# Y = 42.39 -2(GDP) -51.0679
42.39 -51.0679 # -8.6779
# low
-8.6779 -2*(7403.45) # -14815.58
# high
-8.6779 -2*(43055.11) # -86118.9
-14815.58--86118.9
qnorm(1-.05/2)
qnorm(1-.05/2)*(sqrt((.5*.5)/2401.22))
sqrt((.5*.5)/2401.22)
(.5*.5)/2401.22
sqrt((.5*.5)/2401.22)
qnorm(1-.05/2)*(sqrt((.5*.5)/2401.22))
sqrt((.47*.53)/2401.22)
1.96*0.01
2.259*(sqrt((.47*.53)/1745))
2.259*(sqrt((.47*.53)/2402))
1.96*(sqrt((.47*.53)/2402))
qnorm(1-.05/2)*(sqrt((.47*.53)/2402))
qnorm(1-.05/2)*(sqrt((.47*.53)/2403))
qnorm(1-.05/2)*(sqrt((.47*.53)/2405))
qnorm(1-.05/2)*(sqrt((.47*.53)/2410))
qnorm(1-.05/2)*(sqrt((.47*.53)/2400))
qnorm(1-.05/2)*(sqrt((.47*.53)/2300))
qnorm(1-.05/2)*(sqrt((.47*.53)/2350))
qnorm(1-.05/2)*(sqrt((.47*.53)/2375))
qnorm(1-.05/2)*(sqrt((.47*.53)/2400))
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("ggplot2"),  pkgTest)
# set working directory to current parent folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data as vector
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# capture the number of observations
n <- length(y)
# (a) Calculate the 90% confidence interval for the student IQ
# Step 1: get t-score
t <- qt(0.05, n-1, lower.tail = F)
# Step 2: Calculate lower and upper parts for the 90%
lower_CI <- mean(y)-(t*(sd(y)/sqrt(n)))
upper_CI <- mean(y)+(t*(sd(y)/sqrt(n)))
# print CIs with mean
c(lower_CI, mean(y), upper_CI)
# double check our answer
t.test(y, conf.level = 0.9)$"conf.int"
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("ggplot2"),  pkgTest)
# set working directory to current parent folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data as vector
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# capture the number of observations
n <- length(y)
# (a) Calculate the 90% confidence interval for the student IQ
# Step 1: get t-score
t <- qt(0.05, n-1, lower.tail = F)
# Step 2: Calculate lower and upper parts for the 90%
lower_CI <- mean(y)-(t*(sd(y)/sqrt(n)))
upper_CI <- mean(y)+(t*(sd(y)/sqrt(n)))
# print CIs with mean
c(lower_CI, mean(y), upper_CI)
# double check our answer
t.test(y, conf.level = 0.9)$"conf.int"
# (b) Step 1: Calculate the standard error
SE <- sd(y)/sqrt(n)
# Step 2: Calculate the test statistic for this hypothesis testing of mean
t <- (mean(y) - 100)/SE
# Get the p-value from t-distribution
pvalue <- pt(t, n-1, lower.tail = F)
t
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("ggplot2"),  pkgTest)
# set working directory to current parent folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data as vector
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# capture the number of observations
n <- length(y)
# (a) Calculate the 90% confidence interval for the student IQ
# Step 1: get t-score
t <- qt(0.05, n-1, lower.tail = F)
# Step 2: Calculate lower and upper parts for the 90%
lower_CI <- mean(y)-(t*(sd(y)/sqrt(n)))
upper_CI <- mean(y)+(t*(sd(y)/sqrt(n)))
# print CIs with mean
c(lower_CI, mean(y), upper_CI)
# double check our answer
t.test(y, conf.level = 0.9)$"conf.int"
# (b) Step 1: Calculate the standard error
SE <- sd(y)/sqrt(n)
# Step 2: Calculate the test statistic for this hypothesis testing of mean
t <- (mean(y) - 100)/SE
# Get the p-value from t-distribution
pvalue <- pt(t, n-1, lower.tail = F)
# Or another way to do this hypothesis testing is to use the function t.test directly
t.test(y, mu = 100, conf.level = 0.95, alternative = "greater")
# read in expenditure data
expenditure <- read.table("https://raw.githubusercontent.com/ASDS-TCD/StatsI_Fall2023/main/datasets/expenditure.txt", header=T)
# inspect data through summary
summary(expenditure)
# create a matrix scatter plot to
# visualize the relationship among Y, X1, X2 and X3
# so not the first column of expenditure
pdf("plot_2a.pdf")
pairs(expenditure[,2:5], main = "")
dev.off()
# generate boxplot with comparisons for different values of Region
pdf("plot_2b.pdf")
boxplot(expenditure$Y~expenditure$Region, xlab="Region", ylab="Y", main="")
dev.off()
# create scatterplot of Y and X1
# basic and then differentiate color by region
pdf("plot_2c1.pdf", width=8)
ggplot(expenditure, aes(x=X1, y=Y)) +
geom_point() + labs(y="Y\n", x="\nX1") +
theme_bw() +
theme(axis.title = element_text(size=20),
axis.text = element_text(size=15))
dev.off()
# make sure that Region is categorical
expenditure$Region <- as.factor(expenditure$Region)
pdf("plot_2c2.pdf", width=8)
plot(expenditure$X1, expenditure$Y,
pch = as.integer(expenditure$Region),
xlab="X1", ylab="Y", main="",
col = expenditure$Region)
dev.off()
pdf("plot_2c3.pdf", width=8)
ggplot(expenditure, aes(x=X1, y=Y, colour=Region, shape=Region)) +
geom_point()  +
labs(y="Y\n", x="\nX1") +
theme_bw() +
theme(axis.title = element_text(size=20),
axis.text = element_text(size=15),
legend.title = element_text(size=17),
legend.text = element_text(size=15))
dev.off()
cor(expenditure[,2:5])
cormat(expenditure[,2:5])
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("ggplot2"),  pkgTest)
# set working directory to current parent folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data as vector
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# capture the number of observations
n <- length(y)
# (a) Calculate the 90% confidence interval for the student IQ
# Step 1: get t-score
t <- qt(0.05, n-1, lower.tail = F)
# Step 2: Calculate lower and upper parts for the 90%
lower_CI <- mean(y)-(t*(sd(y)/sqrt(n)))
upper_CI <- mean(y)+(t*(sd(y)/sqrt(n)))
# print CIs with mean
c(lower_CI, mean(y), upper_CI)
# double check our answer
t.test(y, conf.level = 0.9)$"conf.int"
# (b) Step 1: Calculate the standard error
SE <- sd(y)/sqrt(n)
# Step 2: Calculate the test statistic for this hypothesis testing of mean
t <- (mean(y) - 100)/SE
# Get the p-value from t-distribution
pvalue <- pt(t, n-1, lower.tail = F)
# Or another way to do this hypothesis testing is to use the function t.test directly
t.test(y, mu = 100, conf.level = 0.95, alternative = "greater")
pvalue
# read in expenditure data
expenditure <- read.table("https://raw.githubusercontent.com/ASDS-TCD/StatsI_Fall2023/main/datasets/expenditure.txt", header=T)
# inspect data through summary
summary(expenditure)
# create a matrix scatter plot to
# visualize the relationship among Y, X1, X2 and X3
# so not the first column of expenditure
pdf("plot_2a.pdf")
pairs(expenditure[,2:5], main = "")
dev.off()
# generate boxplot with comparisons for different values of Region
pdf("plot_2b.pdf")
boxplot(expenditure$Y~expenditure$Region, xlab="Region", ylab="Y", main="")
dev.off()
# create scatterplot of Y and X1
# basic and then differentiate color by region
pdf("plot_2c1.pdf", width=8)
ggplot(expenditure, aes(x=X1, y=Y)) +
geom_point() + labs(y="Y\n", x="\nX1") +
theme_bw() +
theme(axis.title = element_text(size=20),
axis.text = element_text(size=15))
dev.off()
# make sure that Region is categorical
expenditure$Region <- as.factor(expenditure$Region)
pdf("plot_2c2.pdf", width=8)
plot(expenditure$X1, expenditure$Y,
pch = as.integer(expenditure$Region),
xlab="X1", ylab="Y", main="",
col = expenditure$Region)
dev.off()
pdf("plot_2c3.pdf", width=8)
ggplot(expenditure, aes(x=X1, y=Y, colour=Region, shape=Region)) +
geom_point()  +
labs(y="Y\n", x="\nX1") +
theme_bw() +
theme(axis.title = element_text(size=20),
axis.text = element_text(size=15),
legend.title = element_text(size=17),
legend.text = element_text(size=15))
dev.off()
#data
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# First step, look at data
View(y)
head(y)
str(y)
#To find the CI one must have the sample mean +/- t-distribution * standard error
#Mean = the sum of all data points/number of values. 98.44
mean(y)
#Standard error- to find standard error we first need to find the variance and then standard deviation
#To get the Variance we must find the sum of all squared differences (ie.the difference of
#each data point from the mean squared and added together) Divide by n-1. In this case it is 171.4233
var(y)
var_y <- var(y)
#Then, to get the Standard Deviation one must sqrt(variance)  13.09287
sd_y<-sd(y)
sd_y_n <-sqrt(var_y)
#Now that we have Standard Deviation we can calculate Standard Error with the formula
#SE=sd/sqrt(n) it comes out to 2.618575
se_y<-sd(y)/sqrt(length(y))
se_y
#Fnially we must find the t-statistc, we use t instead of z/normal
#because we have a small sample size (<30)
#First we must find degrees of freedom (n-1=24)
#We used .95 because it is two tailed.
qt(.95,df=(length(y)-1))
#Now to find the confidence interval. The formula is mean(y) plus/minus t_score*se
#(98.44 +/- 1.317 * 2.618)
lower_ci <- mean(y) - qt(.95, df = (length(y) - 1)) * se_y
upper_ci <- mean(y) + qt(.95, df = (length(y) - 1)) * se_y
lower_ci
upper_ci
